Our story takes place at Asar, a leading open source developer of conversational AI, usually employed in chatbots. Asar plans an overhaul of a module that is part of their open source framework, called Asar2B, geared particularly to the enterprise market.
Asar’s product manager, Mina, relies on his two developers, Divya and Darnell, to get the job done.
Mina had been pushing to set certain defaults so that would make it easy for the software to exploit affective vulnerabilities in clients. This, in Mina’s words, would ‘optimize’ clients’ buying decisions. In a prototype, this worked fearsomely effectively. 
Customers interacting with the bot had significantly higher purchasing values.
However, fearing a public backlash against their product based on this feature, both Divya and Darnell were able to convince Mina to refrain from doing this.
Unfortunately, however, as was only discovered much later, a part of the code - the one that enabled the function - remained in the program since Divya had missed it in her final review, which accounted for the seeming ‘realness’ of the chatbots developed 
based on Asar2B. This only came to light because Asar2B would cause a big ruckus in the public debate about ethics when a programming whiz called Warda used Asar2B for a purpose it was not originally intended for: realizing its power to appeal to people’s 
emotions, Warda wanted to tap into the large mental health market, so she created an automated mental health adviser called FreudForYou, based on Asar2B. Alas, though the app appeared to understand the concerns of people who interacted with it, it failed 
in its accuracy of being able to do so. It turned out to have been, instead, eliciting stronger and stronger emotions in people.
Of course, none of this was known to Harinder, an avid user of the app. Harinder had been struggling with severe depression for years. As a result of a combination of his depression and use of the app, Harinder took his own life. It was later found out 
that FreudForYou - after an increasingly escalating conversation - had recommended a “heroic end” for him.
Harinder’s grieving family was outraged, and the public jumped on the case. Warda was nowhere to be found, and attention turned to Asar as the culprit.
Journalists urgently demanded a statement from ASAR. Some tracked down the product team led by Mina, doxing their details online.
So what has gone wrong here from an ethical perspective? Determining this as well as a preventative solution will be your task in what follows. This exercise will help you in recognizing and dealing with similar ethical issues when designing and using open
source software in the real world.
Imagine that another developer, “Good-Developer-Gary”, had intended to use the open source software designed by Asar to build his own chat-bat to help users navigate an otherwise complicated banking website. Gary, however, unlike Warda, caught the ethical 
issue with the design that Divya had initially overlooked. What ought Gary to do in this case?
a. Provide feedback to Asar about this so that they may be aware of the issue and work on resolving it
##################################################################################################################################################################################################################################################################################
How might the Asar team go about monitoring their conversational AI?
    d. All of the above
##################################################################################################################################################################################################################################################################################
True or false: There’s a single clear and widely agreed-upon metric for evaluating fairness that Asar should have followed.
    b. False
##################################################################################################################################################################################################################################################################################
How might Mina, Darnell, Divya, and Warda set (or check) safeguards for privacy for the conversational AI? (Select all answers that apply)
    a. By considering what the end-users of the conversational AI will be comfortable sharing
    b. By designing a consent plan
    d. By understanding what the potential privacy threats may be for a conversational AI systems
    e. By considering whether the data that they will be collecting is necessary for achieving the purpose and goals of the product
##################################################################################################################################################################################################################################################################################
Imagine the conversation AI that Warda trained had only been trained on the issues of non-disabled, cis-gendered, heterosexual Caucasian users and had not been trained to deal with, for instance, various forms of discrimination faced by individuals
that fall outside of this group, which as it turns out, is the cause behind a number of the app’s actual users’ mental health issues. This would be a failure to account for which of the following?
    a. Fairness
##################################################################################################################################################################################################################################################################################
Imagine if a pharmaceutical company offered to pay Warda a large sum of money in exchange for the information she has collected on the users using her app. The pharmaceutical company claims that this information will allow them to better reach 
their appropriate target audience so that they may advertise a particular drug to them. Should Warda accept this offer?
    c. No. This would be failing to account for privacy
##################################################################################################################################################################################################################################################################################
How might Asar protect against ethical drift with regard to privacy? (Select all answers that apply)
    a. By monitoring user consent and ensuring that users’ privacy is protected despite any changes that have been made since the product was deployed
    b. By monitoring privacy techniques and checking for any data breaches that may compromise users’ privacy
##################################################################################################################################################################################################################################################################################
What conditions of moral responsibility did our characters have to meet in order to be deemed, to some extent, responsible for the error that played a role in Harinder’s decision to end his life? (Select all answers that apply)
a. The knowledge condition (awareness of the potential harms, their consequences, and alternatives)
b. The freedom condition (ability to influence the outcome)
e. The causal contribution condition (there’s a causal relationship between the actor’s actions and the outcome)
##################################################################################################################################################################################################################################################################################
The failure to account for which of the following resulted in the error that caused Harinder’s decision to end his life?
    c. Wellbeing
##################################################################################################################################################################################################################################################################################
True or false: In evaluating the conversational AI, Mina, Darnell, and Divya should have collaborated with the community of developers.
    a. True. Wellbeing, fairness, and privacy don’t have just one method for evaluation where the conversational AI is concerned. Input from other members would have been beneficial to the evaluation process
##################################################################################################################################################################################################################################################################################
What may be the purpose of Asar’s continuous monitoring of their conversational AI software after it’s published? (Select all answers that apply)
a. To guard against ethical drift
b. To account for changes in the clients’ needs
c. To account for societal changes
d. To account for changes in ethical standards
e. To account for new information coming to light
##################################################################################################################################################################################################################################################################################
Your action plan for wellbeing should take into account:
    e. Both long-term and short-term wellbeing, and if there’s conflict, the conflict must be resolved to satisfy both. If either must be sacrificed for the other and there’s no option to mitigate the harm, then the goals and objectives of the product
must be reconsidered
##################################################################################################################################################################################################################################################################################
Assuming Asar’s team, upon evaluation, had become aware that their conversational AI was likely to exploit client vulnerabilities. How might they have proceeded? (Select all answers that apply)
    a. By detecting the source of the problem and finding that this was due to the code not being altered after deciding against Mina’s initial plan
    c. By devising a plan for resolving this issue
##################################################################################################################################################################################################################################################################################
What’s the main question that the developers of the conversational AI ought to be asking when evaluating the privacy of their system?
    a. Whether the users that will be sharing personal information with the chatbots will have the personal and sensitive information that they share with the chatbots protected
##################################################################################################################################################################################################################################################################################
Assuming the Asar team was not aware of any ethical issues prior to publishing their work, is it reasonable to expect Mina, Divya, and Darnell to ever become aware of the issue with their conversational AI?
    a. Yes. A designer’s work is not done once their work is published. They have to continue being receptive to feedback from the developers using their open source product. If they do this properly, they will be made aware of this and other ethical 
issues by other members of the community
##################################################################################################################################################################################################################################################################################
What steps should Warda have taken to evaluate Asar’s conversational AI after intending to use it to program a mental health adviser? (Select all answers that apply)
    a. Check for wellbeing, privacy, and fairness again prior to building on it in order to ensure that Mina, Divya, and Darnell didn’t overlook anything
    b. Adapt the evaluation to her own product, identifying any new evaluations that need to be added to the list
    c. Provide feedback to Asar in the case that she comes across any ethical concerns
##################################################################################################################################################################################################################################################################################
At which stages should safeguards have been placed/checked for the open source conversational AI? (Select all answers that apply)
    a. At the initial stage, when setting objectives and goals for the system
    b. Prior to setting defaults for the conversational AI
    c. Prior to publicly publishing the code
    d. When creating a mental health app using the code
    e. Upon retiring it
##################################################################################################################################################################################################################################################################################
Assuming that upon evaluation, Asar’s team found that the conversational AI had a bias: the chatbox was providing different advice based on the language style that the user had used to communicate with it (this language style often being tied to the 
ethnic group that a user belongs to). How might Asar proceed? (Select all answers that apply)
    a. By checking the data
   c. By checking the algorithm
   d. By checking their own biases as the developer
##################################################################################################################################################################################################################################################################################
Which character(s) were required to account for ethics prior to making decisions about the technology? (Select all answers that apply)
    a. Mina, the product manager behind the open source product
    b. Divya and Darnell, the developers of the open source product
    c. Warda, the programming whiz, the user of the open source product
##################################################################################################################################################################################################################################################################################
Assuming Warda’s plan of using Asar’s software to develop a mental health chatbot requires users’ vulnerabilities to be known (and thus, risk being potentially exploited). As we’ve seen, this potential exploitation of vulnerabilities results in 
ethical issues that must be avoided, however, to avoid them would require the system to be a much less effective mental health adviser. What might be an appropriate way to proceed?
c. Reconsider the goals and objectives of the product
